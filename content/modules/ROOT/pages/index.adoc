= Building a Code Assistant with Granite and RHEL AI

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

[#intro]
== Introduction to the Lab

Thank you for taking the time to learn about implementing large language model technology to create your own code assistant! We're gonna have a good time, so buckle up!

During this lab experience, you will setup a RHEL AI machine, deploy a large language model (Granite), and then connect this running model to a code assistant extension within VSCode. 

By the end of your lab, you will know how to serve a large language model (LLM) on RHEL AI and implement that LLM technology practically in an active development environment.

[#rhelai]
== What is RHEL AI?

RHEL AI consists of several core components:

. The https://www.ibm.com/granite[Granite] Large Language Model(s)
. https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] for model alignment
. https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/image-mode[RHEL Image Mode]
. Support and indemnification from Red Hat

For the purpose of this lab, we will focus on the functionality of the Granite models, using vLLM within RHEL AI as a model serving runtime, and leveraging the Granite LLM within a standard development environment.

Before we dive in, let's learn a bit more about LLM technology. 

[#llms]
=== What is a Large Language Model?

A large language model (LLM) is a type of artificial intelligence (AI) model that uses deep learning techniques to understand and generate human-like text based on input data. These models are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data. They can be used for various natural language processing (NLP) tasks, such as:

* *Text classification*: Categorizing text based on its content, such as spam detection or sentiment analysis.
* *Text summarization*: Generating concise summaries of longer texts, such as news articles or research papers.
* *Machine translation*: Translating text from one language to another, such as English to French or German to Chinese.
* *Question answering*: Answering questions based on a given context or set of documents.
* *Text generation*: Creating new text that is coherent, contextually relevant, and grammatically correct, such as writing articles, stories, or even poetry.

Large language models typically have many parameters (millions to billions) that allow them to capture complex linguistic patterns and relationships in the data. They are trained on large datasets, such as books, articles, and websites, using techniques like unsupervised pre-training and supervised fine-tuning. Some popular large language models include GPT-4, Llama, and Mistral.

In summary, a large language model (LLM) is an artificial intelligence model that uses deep learning techniques to understand and generate human-like text based on input data. They are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data, and can be used for various natural language processing tasks.

NOTE: To give you an idea of what an LLM can accomplish, the entire previous section was generated with a simple question against the foundational model you are using in this workshop.

[#granite_intro]
== Introducing Granite

IBM’s Granite 3.0 foundational models are cost-efficient, enterprise-grade large language models designed to empower businesses with advanced generative AI and natural language processing (NLP) capabilities. Leveraging IBM’s expertise in AI, data security, and scalable cloud infrastructure, these models prioritize trustworthiness, safety, and reliability for ethical and responsible AI use.

Granite 3.0 models set a high standard in domains like multilinguality, reasoning, coding, and cybersecurity, consistently achieving exceptional benchmark results for their size. The instruct variants enhance capabilities in dialogue, instruction-following, and safety alignment, making them ideal for enterprise use. These models excel in complex tasks such as retrieval-augmented generation (RAG) and regulated applications, all while operating efficiently on constrained compute resources. With robust safety mechanisms, multilingual support, and built-in regulatory compliance, Granite models are particularly suited for industries like healthcare, finance, and government, where performance and data privacy are paramount.

I know this feels like a Granite advertisement, but we want people to start talking about Granite! So, get with the club!

[#granite_models]
=== Granite Models

While I recommend you read the https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf[Granite 3.0 full paper] (not now, later!) let me, for now, give you the low-down.

The Granite 3.0 family offers a range of enterprise-ready language models designed to balance power and efficiency for diverse AI applications. Here’s a quick look:

**Dense Models: Compact yet Powerful**

* Granite-3.0-2B: 2 billion active parameters, ideal for general enterprise tasks with moderate compute needs.

* Granite-3.0-8B: This is the one we're working with today! 8 billion active parameters, ideal for high-performance tasks like reasoning and large-scale data processing.

Dense models are robust and versatile, delivering consistent results with straightforward transformer architectures.

**Mixture-of-Experts (MoE) Models**

* Granite-3.0-1B-A400M: 1 billion parameters (400M active), optimized for resource-limited environments.

* Granite-3.0-3B-A800M: 3 billion parameters (800M active), offering a balance of efficiency and capability.

**Guardian Models**

The Guardian models provide a unique safeguard layer for AI systems:

* Purpose: These models are tailored to detect risks such as harmful or inappropriate outputs and support safe, regulated AI deployment.
* Flexibility: Compatible with both proprietary and open LLMs, enabling wide applicability.
* Guidance: Accompanied by a Responsible Use Guide to help developers build AI responsibly while adhering to industry-specific compliance needs​​.

[#indemnification]
=== Granite Models & Indemnification

Granite models stand out not only for their technical capabilities but also for the robust indemnification we provide for the Granite models included in Red Hat products. This indemnification protects customers against potential legal risks, including intellectual property disputes, copyright issues, and challenges related to AI-generated outputs such as bias or inaccuracies.

This assurance is particularly valuable for organizations in regulated industries or those handling sensitive data, ensuring peace of mind while deploying AI solutions. By standing behind Granite models both technically and legally, IBM and Red Hat reinforce the commitment to trustworthy and secure enterprise AI.

[#code_asst_intro]
=== What is a Code Assistant?

We're here to build a code assistant today. We might think we understand exactly what a code assistant is or does. But in case you have never used one before, let me give you a little introduction:

Generative AI code assistants are tools powered by advanced large language models (LLMs) that help developers write, debug, and optimize code. They’re trained on vast and diverse codebases and technical documentation, enabling them to understand and generate human-readable code. These assistants then integrate seamlessly into your development environments to act as dynamic, context-aware collaborators!

Here’s how they work:

* Understanding Context: These tools analyze the input provided by the developer, whether it’s a natural language description of a task, an existing code snippet, or an error message.
* Code Generation: Based on the input, they predict and generate relevant code, offer solutions, or even rewrite code for improved performance or readability.
* Pattern Synthesis: They generate or refactor code by recognizing patterns in existing data, ensuring it aligns with frameworks and follows coding best practices.

Common Use Cases:

* Code Optimization: Identifying inefficiencies and suggesting performant alternatives.
* Error Diagnosis: Parsing logs or error messages to pinpoint root causes and recommend fixes.
* Automating Documentation: Generating comments, inline explanations, or high-level summaries of code logic.
* Accelerating Testing: Writing unit tests or mocking data for rapid validation of functionality.

Sounds pretty useful - right? Let's go!

[#getting_started]
== Getting Started

=== Environment Details

This lab is comprised of two primary system components: a RHEL AI instance (leveraging Image Mode RHEL technology) and a RHEL 9.4 machine with the Visual Studio Code (VSCode) application installed.

In your user interface, you have two tabs that will be used to the right of these instructions. The first tab, titled **VSCode** shows the VSCode application where you will use the code assistant extension that leverages the deployed Granite model.

The second tab, titled **Terminals**, is for our two terminal windows. We will use these terminal windows to access both the RHEL AI server and the RHEL server with VSCode installed. 

Our first step is to setup our RHEL AI machine and deploy the Granite model. 

[#ssh_rhelai]
=== Connecting to RHEL AI

Navigate to the second **Terminals** tab.

The terminals are currently connected to the RHEL machine hosting VSCode, so we must SSH into the RHEL AI instance.

You will use both terminal windows during the lab, so go ahead and SSH into both windows.

From each terminal, enter: `ssh rhelai` to authenticate to the RHEL AI server. You will not need a password!

// double verify if the below will be fixed for 1.3
Due to a product constraint, you must run all commands as root.

To run every command as root, enter the following command:

[source,console,role=execute,subs=attributes+]
----
sudo su -
----

[#verify_ilab]
=== Verify ilab Installation
//may remove if tony implements the fix here
Before proceeding, you will need to bypass the prompt to register the device with Red Hat Insights. You can do this by running the following command:

[source,console,role=execute,subs=attributes+]
----
mkdir -p /etc/ilab
touch /etc/ilab/insights-opt-out
----

Now you are good to go to proceed!

'''

RHEL AI includes the ilab CLI tool, pre-installed (remember, RHEL AI uses Image Mode for RHEL packaging so all necessary tools in the product are pre-configured).

This CLI tool and its commands are how we will download and serve our large language model. 

In the **upper** terminal window, type in the following to verify the ilab CLI tool installation:

[source,console,role=execute,subs=attributes+]
----
ilab
----

That was quite of bit of information! Read through it to get acquainted with our ilab CLI tool.

[#initialize_ilab]
=== Initializing InstructLab
// verify below output in 1.3
Before you can do anything with ilab, it must be initialized

In the same terminal window, type the following command to initialize ilab.

[source,console,role=execute,subs=attributes+]
----
ilab config init
----

During the configuration, ilab asks for a couple of file location settings. Press `enter` to accept the defaults.

A few things happen during initialization. A taxonomy is generated, a configuration file (`config.yaml`) is created in the `/root/.config/instructlab/` directory, and the appropriate system profile is selected based on your system hardware details. This system profile contains configuration settings tuned to your hardware for each phase of the instructlab workflow. This is more impactful when you are using the full InstructLab framework (synthetic data generation and training), which we aren't doing in this lab.

Let's take a look at the configuration file that was generated. Enter the following command:

[source,console,role=execute,subs=attributes+]
----
ilab config show
----

Within this configuration you can see all of the default settings. This file can be altered based on a customer’s needs. However, we do not want to encourage customers to adjust many of the settings in this file.

[#download]
== Download the Model from the Registry

Now that we have initialized ilab, we need to download the model we will use for our code assistant.

In RHEL AI, you can download and deploy any model for inferencing that vLLM supports. However, for a Red Hat-supported experience, you must download the Granite models from the official Red Hat container registry. This will require authentication to our registry.

[#svc_account]
=== Authenticating to the Red Hat Container Registry

In order to login to the container registry, you need a service account. To save you some time, we will provide you a username and password to use:

**Username**: 11009103|rhone-code

**Password Token**: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiJiOGUwYjFkYzJlMWM0MDE4YjUxZDNkODFiMzQyNTI4YSJ9.NTU_z813egTBmmiDUiVWfgC9X8lL4VGCDEPF9FrJo8fk7-qPgKCjeQj59gLakD-rCpTnmiNbiQABDHe5k_MXUmBAS17-h1Z8HtrGJHXXGjbx3DvRRO1O5Ennr4avoO1MLdM_mX5ZXq9sSLNZUpWgtCh8lI6L-6LBT_mWhQdf2TH5i2UCF9_H1-_IL4vnphzXJRxrXeeKP7Bw72S9kzG-PSceYJVkrq7GQr4TJbN_Pcy36Ov7jGQkc5yYTKB-2QZxc5yKfq_mJI8vz1Y62zUIXpd3r7Hgisvl-aHbgdC3d96vnJBHwY483zr6zYLs0t_hK45om59ASevEuT-8DdqGl53Wgh1iaLDwDoX23g6SoZs6jguZG4aL-Trg2zAibta2iwVu0EXqyCLTv3tI6kginuA9JAVzeo0WlarzgEzjDNNMb1nThFFUODQZRnRJ0Jz8RZ3AsrGTpYGh7ojhE__1y4sS6yxM9Zqpul7xqaPsVsYY_D_SWdY_Qv5sp-5nF-PcQV4s6C88LSgcuuJ7QGxtLkgN9B7s6R8mNwo6fEyZ9ecpmR_eEW8p5itKy9uV2zqi0kaM4QnFsHS0wHSnTzV1WKsMynW1efs5e--UHSk6poqarT8afVz0SIVq89cN9VKUxOmzWKLkTlycVBxu_1fDBOHUJT_ofizJq0dPpGOoo40

NOTE: If you would like to create your own service account, navigate to https://access.redhat.com/terms-based-registry/ and login (SSO) to create a new service account. Follow the steps to create a new account. Once created, you can search for your newly created account by searching for your name in the search bar.

Now that you have credentials to the registry, you need to authenticate your RHEL AI machine.

From the command line, enter:

[source,console,role=execute,subs=attributes+]
----
podman login registry.redhat.io
----

Enter the login credentials as prompted. When successful,  you should see a response of `“Login Succeeded!”`

You are now ready to start downloading models.

[#dl_model]
=== Downloading the Granite Model

Now that you have ilab initialized and you are logged into the registry, you can download the Granite model that we will use for our coding assistant: granite-8b-lab-v1 . 

Enter the following command:

//fix below command with proper registry download command once we have the image from 1.3.1 with updated vllm version / everything

// ilab model download --repository docker://registry.redhat.io/rhelai1/granite-8b-lab-v1 --release latest
// ----

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository docker://registry.redhat.io/rhelai1/granite-7b-redhat-lab --release latest
----

The download will take several minutes to complete and will not display progress. Don't dismay, it's happening! You'll know the model is downloaded once you see the shell prompt available again.

Once the download completes, enter `ilab model list` into the terminal. You should see results as in the image below.
// again fix below with correct name
[source,console]
----
+-----------------------------------+---------------------+---------+
| Model Name                        | Last Modified       | Size    |
+-----------------------------------+---------------------+---------+
| models/granite-7b-redhat-lab         | 2024-09-24 14:40:57 | 12.6 GB |
+-----------------------------------+---------------------+---------+
----

[#serve_model]
== Serving the Model

Now that we downloaded the Granite model, you have a model that you can serve and chat with locally. Before integrating it into our development environment, let's chat with it, as is, within RHEL AI.

Enter the following command into one of the terminals to serve the Granite 8b model. You do not have to run this command separately, you could just run the chat command, but I want you to see this full process execute.

//fix name
[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /root/.cache/instructlab/models/granite-7b-redhat-lab
----

It typically takes a few moments for vLLM to start. This is expected. When you see the following output, you will be able to continue.

[source,console]
----
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
----

[#vllm]
=== All about vLLM

vLLM is the primary inference engine supported in RHEL AI, and a model runtime option also available in OpenShift AI. Here's a quick look at this technology:

vLLM is an open-source library designed to optimize the inference of LLMs. It uses a novel memory management approach, PagedAttention, to reduce memory usage and increase throughput. By dynamically batching incoming requests, vLLM ensures efficient hardware utilization and lower latency.

**Benefits of vLLM:**

* High Efficiency: Achieves significantly higher throughput compared to standard LLM serving methods.
* Resource-Friendly: Reduces memory requirements, making it ideal for constrained environments.
* Easy Integration: Compatible with OpenAI's API, simplifying adoption into existing workflows.
* Supported Models: vLLM supports a wide range of generative and embedding models, including Aquila, BaiChuan, ChatGLM, and Bloom-based architectures. This flexibility makes it suitable for diverse AI applications.

[#chat]
=== Chat with the Model

Now you will utilize your second terminal window to chat with the deployed model.

Once the model server is up and running, enter the following commands in the **unused** terminal window in order to chat with the Granite model you just downloaded. 

//again edit this out if this isn't needed in 1.3.1
First, ensure you are running as root in this terminal window:

[source,console,role=execute,subs=attributes+]
----
sudo su -
----

Now enter the `ilab model chat` command:
//adjust for proper command
[source,console,role=execute,subs=attributes+]
----
ilab model chat --model /root/.cache/instructlab/models/granite-7b-redhat-lab
----

You will know you are successful when the following appears on the screen:
//adjust output to reflect 8b
[source,console]
----
╭─────────────────────────────────── system ──────────────────────────────────╮
│ Welcome to InstructLab Chat w/ GRANITE-7B-REDHAT-LAB (type /h for help)        │
╰─────────────────────────────────────────────────────────────────────────────╯
>>>                                                                 [S][default]
----

At the chat prompt (`>>>`), feel free to chat with the model a bit. See what it knows! 

[#code_asst]
== Integrating the Granite Model into a Code Assistant

So, we have our model deployed and we've chatted with it a bit in RHEL AI - awesome! Now let's get our code assistant setup so we can do some real development work.

[#api]
=== Setup vLLM API Key

Before we go to our Visual Studio Code environment, we'll need an api key which we will use to access our deployed model. Exit out of the chat if you still have it running by typing `exit`.

NOTE: The following instructions are also in our https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.3/html/building_your_rhel_ai_environment/serving_and_chatting#creating_api_key[official documentation]!

1. Create an API key that is held in $VLLM_API_KEY parameter by running the following command in a terminal window:

[source,console,role=execute,subs=attributes+]
----
export VLLM_API_KEY=$(python -c 'import secrets; print(secrets.token_urlsafe())')
----

2. View your API key
[source,console,role=execute,subs=attributes+]
----
echo $VLLM_API_KEY
----
//look at figuring out a better instruction as it's so easy to do this incorrectly
Please note this api-key string separately as copy/paste within TMUX session will not work. 

3. Edit the config.yaml 
[source,console,role=execute,subs=attributes+]
----
ilab config edit
----

4. Add the following params to the `vllm_args` section of the `config.yaml` file with **your api-key**:
[source,console,role=execute,subs=attributes+]
----
serve:
    vllm:
        vllm_args:
        - --api-key
        - <api-key-string>
----

5. Change the default host_port from `127.0.0.1:8000` to `0.0.0.0:8000` in the `config.yaml`:
[source,console,role=execute,subs=attributes+]
----
serve:
    host_port: 0.0.0.0:8000
----

6. Get your ip address
[source,console,role=execute,subs=attributes+]
----
ip -o -4 a
----

Take the `:eth0 inet` value. Example: `192.168.0.11`. Take note of this value for the next step.

7. Verify the RHEL AI server is using API key authentication by running the following command:
[source,console,role=execute,subs=attributes+]
----
curl -s -H "Authorization: Bearer 12345" http://<your-ip-address>:8000/v1/models
----

This should show the following error:

[source,console]
----
{"error":"Unauthorized"}[root@rhelai ~]#
----

8. Verify the API key works properly

Now, ensure it's working as it should with the right curl command:
[source,console,role=execute,subs=attributes+]
----
curl -s -H "Authorization: Bearer $VLLM_API_KEY" http://<your-ip-address>:8000/v1/models
----

You should get output similar to the following:

[source,console]
----
{"object":"list","data":[{"id":"/root/.cache/instructlab/models/granite-7b-redhat-lab","object":"model","created":1733947948,"owned_by":"vllm","root":"/root/.cache/instructlab/models/granite-7b-redhat-lab","parent":null,"max_model_len":4096,"permission":[{"id":"modelperm-17738edd57994607b5025cf8eac1919e","object":"model_permission","created":1733947948,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}[root@rhelai ~]#
----

Amazing! We have proven authorized access. Now we're ready to get our code extension up and running!


[#vscode]
=== Visual Studio Code

Navigate back to the first **VSCode** tab to begin working in the Visual Studio Code application.

[#install_asst]
=== Install Code Assistant Extension

Select the bottom navigation item on the left-hand side to open up the extensions marketplace.

image::extensions_tab.png[width=100%]

In the search bar, search for **Paver**, which is currently the name of our code assistant!

You'll see Paver as the top option, as shown below:

image::paver.png[width=100%]

Click the **arrow** next to the Install and select `Install Pre-Release Version`.

image::pre-release.png[width=100%]

Give it a sec to install, then you'll know it's installed successfully when you see the setup wizard:

image::setup.png[width=100%]

[#setup_asst]
=== Setup our Code Assistant

As of today, this code assistant extension works natively with Ollama and Continue. However, since we will be using a model that is deployed on RHEL AI for inferencing, we don't need Ollama! So let's setup our RHEL AI connection.

Continue is installed when you install the Paver extension. Navigate to the Continue extension in the left-hand side navigation bar:

image::continue_tab.png[width=100%]

In order to add the Granite model deployed on RHEL AI as a model option in Continue, we need to edit the `config.yaml` file. 

Click on the **Settings* icon of the Continue extension to open the config.json file:

image::settings_icon.png[width=100%]

Now, in the `config.json` file, in the `models` section, add the following:
//fix below with 3.0 model

[source,console,role=execute,subs=attributes+]
----
{
  "title":RHEL AI Granite 3.0 8B",
  "provider": "openai",
  "model": "granite-7b-redhat-lab",
  "apiBase":"http://<your-ip-address>:8000/v1"
  "apiKey": "<your-api-key>"
},
----

Continue will automatically reset and you will see the Granite model from your RHEL AI server as an option in the drop-down as seen below:

image::our_model.png[width=100%]

We did it! Now, let's select and use the Granite model on a real project.

[#code_activity]
== Using the Code Assistant

// activity to utilize the code assistant and see functionality

// curl down a code file from github
// do something with extension

[#conclusion]
== Conclusion

Great job! 

Our goal today was to have you familiarize yourself with the RHEL AI environment and the Granite models. We wanted you to see how to utilize this technology in a practical, repeatable way.

Code assistant technology is a prominent use case with customers, and we hope the lab today helped you envision how you can work with customers to leverage the Granite LLMs and Red Hat AI for a secure, flexible, reliable code assistant solution.

Now that you've gotten your hands dirty, you might be wondering what you can do next to up your game.

//suggestions on further learning
// red hat ai links
// granite links